{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperimantiranje"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prvo ćemo importati potrebne pakete i fiksirati _random seed_ kako bi kod bio reproducibilan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "random_state = 42\n",
    "rng = np.random.RandomState(random_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sljedeća funkcija služi generiranju uzorka jediničnih normalnih veličina $x$ i $y$ td.\n",
    "\\begin{align*}\n",
    "y &= \\beta x + \\varepsilon, \\ \\ \\varepsilon \\sim N(0, \\sigma^2) \\\\\n",
    "\\mathrm{Cov}(x_i, x_j) &= \\sigma^2 > 0, \\ \\ i \\neq j.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(beta: np.array, rho: float, sample_size: int, error_var: float = 1):\n",
    "\n",
    "    \"\"\"Generates sample of X and y such that y = X*beta + eps, eps ~ N(0, sqrt(error_var))\n",
    "\n",
    "    Args:\n",
    "        beta (np.array): Linear transformation vector.\n",
    "        rho (float): Covariance between covariates.\n",
    "\n",
    "    Returns:\n",
    "        sample: generated sample.\n",
    "    \"\"\"\n",
    "\n",
    "    N = beta.shape[0]\n",
    "    cov = rho * np.ones([N, N])\n",
    "    cov = cov + np.diag(1-rho * np.ones(N))\n",
    "\n",
    "    N = beta.shape[0]\n",
    "    X = rng.multivariate_normal(mean = np.zeros(N), cov=cov, size=sample_size)\n",
    "\n",
    "    error = rng.normal(loc=0, scale=np.sqrt(error_var), size=X.shape[0])\n",
    "    y = np.matmul(X, beta) + error\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brzinski _sanity check_: ako je $\\rho = 0,$ tada je $$\\mathrm{Var}(y) = \\sum_{k=0}^N \\beta_k^2 \\mathrm{Var}(X_k) + \\mathrm{Var}(\\varepsilon),$$ što bi u slucaju $N = 3, \\beta = (1, \\dots, 1)$ moralo biti $4$. Zaista, za dovoljno velik uzorak imamo da je uzoračka varijanca $\\approx 4$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Var(y) = 3.9966141119858167\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "beta = np.ones(3)\n",
    "rho = 0\n",
    "sample_size = 1000000\n",
    "\n",
    "X, y = get_sample(beta=beta, rho=rho, sample_size=sample_size)\n",
    "print(f'>>> Var(y) = {y.var(ddof=1)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nastavljamo definiranjem funkcija za treniranje LS, PCR i PLS modela, pri čemu potonja dva kao parametar primaju i broj glavnih komponenti koje koriste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(str, Enum):\n",
    "    linreg = \"linreg\"\n",
    "    pcr = \"pcr\"\n",
    "    pls = \"pls\"\n",
    "\n",
    "    @staticmethod\n",
    "    def train(model_name: str, X: np.array, y: np.array, n_components: int | None = None):\n",
    "\n",
    "        if model_name not in list(Model):\n",
    "            raise ValueError(f'No such model. Available models are {list(Model)}')\n",
    "        \n",
    "        if model_name == Model.linreg:\n",
    "            model = LinearRegression()\n",
    "        \n",
    "        elif model_name == Model.pcr:\n",
    "            model = make_pipeline(PCA(n_components=n_components, random_state=random_state), LinearRegression())\n",
    "        \n",
    "        elif model_name == Model.pls:\n",
    "            model = PLSRegression(n_components=n_components)\n",
    "        \n",
    "        model.fit(X, y)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedan razuman način validacije naših modela bio bi da koristeći distribucije iz kojih smo generirali podatke izračunamo populacijski $\\beta$ pa za grešku modela uzmemo\n",
    "koliko se njegov koeficijent razlikuje od populacijskog, tj. ako je nas model dan s $y = \\hat{\\beta}x$, njegovu grešku možemo računati kao\n",
    "\\begin{align*}\n",
    "\\mathrm{Err}(\\mathrm{Model}) = \\|\\beta-\\hat{\\beta}\\|.\n",
    "\\end{align*}\n",
    "Međutim, kako je prilikom visoke korelacije kovarijata taj $\\beta$ \"nestabilan\", mi ćemo umjesto toga testirati naše modele na velikom testnom uzorku. Preciznije, prvo ćemo izgenerirati jako velik uzorak, zatim trenirati model na njegovom malom dijelu, a na ostatku izračunati $R2,$ što ce nam biti primarna metrika za validaciju modela. Takav pristup ima dvije prednosti:\n",
    "1. Metrike poput kvaratne greške i $R2$ su interpretabilnije od udaljenosti do stvarnog $\\beta$.\n",
    "2. Tako se stvari rade u praksi (jer ne znamo stvarne distribucije pa ni vrijednost populacijskog koeficijenta); istrenira se model na uzorku koji nam je dan, a zatim validira na testnom skupu pa ide u produkciju.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 50_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_all_models(X_train: np.array, X_test: np.array, y_train: np.array, y_test: np.array, n_components: int):\n",
    "    \n",
    "    \"\"\"Trains LS, PLS and PCR models on X_train and y_test and evaluates them on X_test and y_test.\n",
    "\n",
    "    Args:\n",
    "        n_components (int): Number of relevant components to use for predicition in PCR and PLS.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains model names with their respective R2 values.\n",
    "    \"\"\"\n",
    "\n",
    "    score_dict = {}\n",
    "\n",
    "    for model_name in list(Model):\n",
    "        model = Model.train(model_name=model_name, X=X_train, y=y_train, n_components=n_components)\n",
    "        score_dict[model_name.value] = model.score(X_test, y_test)\n",
    "\n",
    "    return score_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donji primjer pokazuje kako kod \"skoro\" nezavisnih kovarijata PLS bolje predviđa nego PCR. To je i očekivano, pogotovo ako je broj komponenti puno manji od $N$ jer tada PCA nužno gubi bitne informacije za predviđanje. S druge strane, PLS, rastavljajuci zavisnu varijablu skupa s nezavisnima, ne izgubi gotovo ništa te predviđa jednako dobro kao i linearna regresija, ali u puno manjoj dimenziji pa je stoga interpretabilniji od nje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 300\n",
    "train_sample_size = 1000\n",
    "n_components = 5\n",
    "rho= 0.1\n",
    "\n",
    "beta = rng.normal(loc=0, scale=25, size=N)\n",
    "X, y = get_sample(beta=beta, rho=rho, sample_size=SAMPLE_SIZE)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=train_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linreg': 0.9999894545075303,\n",
       " 'pcr': 0.005928099252595187,\n",
       " 'pls': 0.9946321320636914}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict = train_and_evaluate_all_models(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, n_components=n_components)\n",
    "score_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sad ćemo za razne parametre provesti eksperiment i podatke o R2 spremiti u datoteku _scores.csv_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [14:30<00:00, 174.03s/it, N=5, linreg=0.999, n_components=2, pcr=0.996, pls=0.997, rho=0.99, train_sample_size=5]      \n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 100_000\n",
    "\n",
    "index_columns = ['train_sample_size', 'N', 'n_components', 'rho']\n",
    "score_df = pd.DataFrame(None, columns=index_columns+[x.value for x in Model])\n",
    "\n",
    "Ns = [500, 100, 50, 10, 5]\n",
    "loader = tqdm(Ns)\n",
    "\n",
    "N_train_sample_size_ratios = [10, 5, 3, 2, 1]\n",
    "n_components_N_ratios = [0.01, 0.1, 0.25, 0.5]\n",
    "rhos = [0.01, 0.1, 0.2, 0.5, 0.9, 0.99]\n",
    "\n",
    "for N in loader:\n",
    "    for rho in rhos:\n",
    "        \n",
    "        beta = rng.normal(loc=0, scale=25, size=N)\n",
    "        X, y = get_sample(beta=beta, rho=rho, sample_size=SAMPLE_SIZE)\n",
    "\n",
    "        for train_sample_size in [N*x for x in N_train_sample_size_ratios]:\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, train_size=train_sample_size)\n",
    "\n",
    "            for n_components in [int(N*x) for x in n_components_N_ratios]:\n",
    "\n",
    "                if n_components == 0:\n",
    "                    continue\n",
    "\n",
    "                score_dict = train_and_evaluate_all_models(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, n_components=n_components)\n",
    "                hparams_dict = dict(train_sample_size=train_sample_size, N=N, rho=rho, n_components=n_components)\n",
    "                new_row = hparams_dict | score_dict\n",
    "\n",
    "                score_df.loc[len(score_df), :] = new_row\n",
    "\n",
    "                loader.set_postfix(**new_row)\n",
    "        \n",
    "        score_df.to_csv('scores.csv', index=False)\n",
    "\n",
    "score_df.to_csv('scores.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (v3.10.4:9d38120e33, Mar 23 2022, 17:29:05) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1aa0b08baf84a59db0468fc0305201b2935aa2c34c459b1c28b215e727584339"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
